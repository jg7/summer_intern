{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. import packages\n",
    "import spacy\n",
    "import os\n",
    "from collections import Counter\n",
    "import torch\n",
    "import glob\n",
    "from packages.functions import parse_cnn, word_list_to_idx_list\n",
    "from spacy import attrs\n",
    "import numpy as np\n",
    "vocab_size = 500\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load nlp model & files to read\n",
    "nlp = spacy.load('en') # loads default English object\n",
    "cnn_dir = '/home/jatin/intern/internenv/cnn/stories/'\n",
    "cnn_pre_dir = '/home/jatin/intern/internenv/cnn/stories_idx/'\n",
    "file_list = [os.path.join(cnn_dir,file) for file in os.listdir(cnn_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary created from 10/92579 files, top 501 words saved\n",
      "Vocabulary created from 20/92579 files, top 501 words saved\n",
      "Vocabulary created from 30/92579 files, top 501 words saved\n",
      "Vocabulary created from 40/92579 files, top 501 words saved\n",
      "Vocabulary created from 50/92579 files, top 501 words saved\n",
      "Vocabulary created from 60/92579 files, top 501 words saved\n",
      "Vocabulary created from 70/92579 files, top 501 words saved\n",
      "Vocabulary created from 80/92579 files, top 501 words saved\n",
      "Vocabulary created from 90/92579 files, top 501 words saved\n"
     ]
    }
   ],
   "source": [
    "#2. create dictionary using frequent words\n",
    "body_list = []\n",
    "summary_list = []\n",
    "counter = Counter()\n",
    "batch_no = 0\n",
    "while batch_no<len(file_list[200]):\n",
    "    batch = file_list[batch_no:min(batch_no+batch_size,len(file_list))]\n",
    "    for file in batch:\n",
    "        body_words, summary_words = parse_cnn(file,nlp)\n",
    "        body_list.extend(body_words)\n",
    "        summary_list.extend(summary_words)\n",
    "    c = Counter(body_list+summary_list)\n",
    "    counter = counter + c\n",
    "    vocab_list = counter.most_common(vocab_size)\n",
    "    word2idx = dict()\n",
    "    word2idx['<PAD>']=0\n",
    "    word2idx['<S>']=1\n",
    "    word2idx['</S>']=2\n",
    "    word2idx['<UNK>']=3\n",
    "    idx2word = dict()\n",
    "    idx2word[0] = '<PAD>'\n",
    "    idx2word[1] = '<S>'\n",
    "    idx2word[2] = '</S>'\n",
    "    idx2word[3] = '<UNK>'\n",
    "    for i,(word,_) in enumerate(vocab_list):\n",
    "        if len(word2idx)>vocab_size:\n",
    "            break\n",
    "        word2idx[word] = i+4\n",
    "        idx2word[i+4] = word\n",
    "    np.save('word2idx.npy',word2idx)\n",
    "    np.save('idx2word.npy',idx2word)\n",
    "    batch_no+=batch_size\n",
    "    print(\"Vocabulary created from %d/%d files, top %d words saved\" \n",
    "          %(batch_no,len(file_list),len(word2idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 files processed so far\n",
      "2000 files processed so far\n"
     ]
    }
   ],
   "source": [
    "w2i = np.load('word2idx.npy').item()\n",
    "i2w = np.load('idx2word.npy').item()\n",
    "v = len(w2i)\n",
    "# 3. preprocess each document in CNN so that we get a form where a text is seen in vectors\n",
    "out_file_list = [os.path.join(cnn_pre_dir,file) for file in os.listdir(cnn_dir)]\n",
    "in_out_zip = zip(file_list, out_file_list)\n",
    "cnt = 0\n",
    "for in_file, out_file in in_out_zip:\n",
    "    body_words, summary_words = parse_cnn(in_file, nlp)\n",
    "    body_idx = word_list_to_idx_list(body_words, w2i, v)\n",
    "    body_idx = [str(x) for x in body_idx]\n",
    "    summary_idx = word_list_to_idx_list(summary_words,w2i,v)\n",
    "    summary_idx = [str(x) for x in summary_idx]\n",
    "    out = ' '.join(body_idx)+\"::\"+' '.join(summary_idx)\n",
    "    with open(out_file,'w') as f:\n",
    "        f.write(out)\n",
    "    cnt+=1\n",
    "    if cnt%1000==0:\n",
    "        print('%d files processed so far' %(cnt))\n",
    "    # small file batch    \n",
    "    if cnt>2000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'michel',\n",
       " 'platini',\n",
       " 'reiterates',\n",
       " 'his',\n",
       " 'preference',\n",
       " 'for',\n",
       " 'extra',\n",
       " 'officials',\n",
       " 'over',\n",
       " 'technology',\n",
       " 'in',\n",
       " 'football',\n",
       " '.',\n",
       " ' ',\n",
       " 'uefa',\n",
       " 'president',\n",
       " 'says',\n",
       " 'the',\n",
       " 'cost',\n",
       " 'of',\n",
       " 'installing',\n",
       " 'goal',\n",
       " '-',\n",
       " 'line',\n",
       " 'technology',\n",
       " 'is',\n",
       " 'prohibitive',\n",
       " '.',\n",
       " ' ',\n",
       " 'glt',\n",
       " 'was',\n",
       " 'introduced',\n",
       " 'in',\n",
       " 'the',\n",
       " 'english',\n",
       " 'premier',\n",
       " 'league',\n",
       " 'at',\n",
       " 'the',\n",
       " 'start',\n",
       " 'of',\n",
       " 'the',\n",
       " '2013',\n",
       " '-',\n",
       " '14',\n",
       " 'season',\n",
       " '.',\n",
       " ' ',\n",
       " 'platini',\n",
       " 'defends',\n",
       " 'uefa',\n",
       " \"'s\",\n",
       " 'decision',\n",
       " 'to',\n",
       " 'expand',\n",
       " 'the',\n",
       " 'european',\n",
       " 'championships',\n",
       " 'to',\n",
       " '24',\n",
       " 'teams',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_list[0]) as f:\n",
    "    text = f.read()\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n\\n',' ')\n",
    "    text = text.replace('(cnn)','')\n",
    "    text = text.split(\"@highlight\")\n",
    "    body = text[0]\n",
    "    body_tokens = nlp(body)\n",
    "    summaries = text[1:]\n",
    "    summary_tokens = nlp(' '.join([x.strip()+'.' for x in summaries])+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = dict()\n",
    "w2i['<PAD>']=0\n",
    "w2i['<S>']=1\n",
    "w2i['</S>']=2\n",
    "\n",
    "i2w = dict()\n",
    "i2w[0]='<PAD>'\n",
    "i2w[1]='<S>'\n",
    "i2w[2]='</S>'\n",
    "\n",
    "for i,word in enumerate(word2idx):\n",
    "    if len(w2i)>500:\n",
    "        break\n",
    "    w2i[word] = i+3\n",
    "    i2w[i+3] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<PAD>',\n",
       " 1: '<S>',\n",
       " 2: '</S>',\n",
       " 3: 'like',\n",
       " 4: 'small',\n",
       " 5: 'year',\n",
       " 6: 'equality',\n",
       " 7: 'open',\n",
       " 8: 'most',\n",
       " 9: 'got',\n",
       " 10: 'al',\n",
       " 11: 'go',\n",
       " 12: 'because',\n",
       " 13: 'process',\n",
       " 14: 'election',\n",
       " 15: 'state',\n",
       " 16: 'about',\n",
       " 17: 'technology',\n",
       " 18: 'defense',\n",
       " 19: 'rights',\n",
       " 20: 'called',\n",
       " 21: 'show',\n",
       " 22: 'today',\n",
       " 23: 'as',\n",
       " 24: 'days',\n",
       " 25: 'be',\n",
       " 26: ':',\n",
       " 27: 'e',\n",
       " 28: 'issues',\n",
       " 29: 'act',\n",
       " 30: 'under',\n",
       " 31: 'expected',\n",
       " 32: 'policy',\n",
       " 33: 'did',\n",
       " 34: 'can',\n",
       " 35: 'au',\n",
       " 36: 'place',\n",
       " 37: 'by',\n",
       " 38: 'movement',\n",
       " 39: 'to',\n",
       " 40: 'women',\n",
       " 41: 'will',\n",
       " 42: 'amazon',\n",
       " 43: 'two',\n",
       " 44: 'has',\n",
       " 45: 'official',\n",
       " 46: 'past',\n",
       " 47: '2010',\n",
       " 48: 'against',\n",
       " 49: 'sen',\n",
       " 50: 'those',\n",
       " 51: 'department',\n",
       " 52: 'thomas',\n",
       " 53: '2008',\n",
       " 54: 'held',\n",
       " 55: 'day',\n",
       " 56: 'university',\n",
       " 57: 'of',\n",
       " 58: 'million',\n",
       " 59: 'stewart',\n",
       " 60: 'areas',\n",
       " 61: 'does',\n",
       " 62: 'never',\n",
       " 63: 'deal',\n",
       " 64: 'such',\n",
       " 65: 'forces',\n",
       " 66: 'camps',\n",
       " 67: 'support',\n",
       " 68: 'right',\n",
       " 69: 'would',\n",
       " 70: 'gift',\n",
       " 71: 'members',\n",
       " 72: 'through',\n",
       " 73: 'reports',\n",
       " 74: 'cnn',\n",
       " 75: 'been',\n",
       " 76: 'what',\n",
       " 77: 'for',\n",
       " 78: 'know',\n",
       " 79: 'us',\n",
       " 80: 'event',\n",
       " 81: 'over',\n",
       " 82: 'control',\n",
       " 83: 'race',\n",
       " 84: 'percent',\n",
       " 85: 'sure',\n",
       " 86: 'heard',\n",
       " 87: 'me',\n",
       " 88: 'campaign',\n",
       " 89: 'terrorist',\n",
       " 90: 'photos',\n",
       " 91: 'mccain',\n",
       " 92: 'and',\n",
       " 93: 'her',\n",
       " 94: 'means',\n",
       " 95: 'high',\n",
       " 96: 'va',\n",
       " 97: 'there',\n",
       " 98: 'a',\n",
       " 99: 'its',\n",
       " 100: 'major',\n",
       " 101: 'authorities',\n",
       " 102: 'another',\n",
       " 103: 'this',\n",
       " 104: 'republican',\n",
       " 105: 'team',\n",
       " 106: \"'m\",\n",
       " 107: 'storm',\n",
       " 108: 'any',\n",
       " 109: 'some',\n",
       " 110: 'between',\n",
       " 111: 'cherokee',\n",
       " 112: 'majority',\n",
       " 113: 'only',\n",
       " 114: 'outside',\n",
       " 115: 'monday',\n",
       " 116: 'still',\n",
       " 117: 'hit',\n",
       " 118: 'help',\n",
       " 119: 'you',\n",
       " 120: '-',\n",
       " 121: 'while',\n",
       " 122: 'earlier',\n",
       " 123: 'based',\n",
       " 124: 'sterling',\n",
       " 125: 'history',\n",
       " 126: \"'ve\",\n",
       " 127: 'way',\n",
       " 128: 'nation',\n",
       " 129: 'do',\n",
       " 130: 'long',\n",
       " 131: 'get',\n",
       " 132: 'school',\n",
       " 133: 'trying',\n",
       " 134: 'local',\n",
       " 135: 'constitution',\n",
       " 136: 'best',\n",
       " 137: 'without',\n",
       " 138: 'issue',\n",
       " 139: 'we',\n",
       " 140: 'better',\n",
       " 141: '2007',\n",
       " 142: ' ',\n",
       " 143: 'come',\n",
       " 144: 'istanbul',\n",
       " 145: 'killed',\n",
       " 146: 'statement',\n",
       " 147: 'security',\n",
       " 148: 'three',\n",
       " 149: 'is',\n",
       " 150: 'leader',\n",
       " 151: 'wrote',\n",
       " 152: 'more',\n",
       " 153: 'medical',\n",
       " 154: 'now',\n",
       " 155: 'hurricane',\n",
       " 156: 'anti',\n",
       " 157: 'really',\n",
       " 158: 'tuesday',\n",
       " 159: 'men',\n",
       " 160: 'among',\n",
       " 161: 'man',\n",
       " 162: 'where',\n",
       " 163: '1',\n",
       " 164: 'out',\n",
       " 165: 'spokesman',\n",
       " 166: 'book',\n",
       " 167: 'contributed',\n",
       " 168: 'woman',\n",
       " 169: 'their',\n",
       " 170: 'early',\n",
       " 171: 'take',\n",
       " 172: 'put',\n",
       " 173: 'paperwhite',\n",
       " 174: 'saying',\n",
       " 175: 'years',\n",
       " 176: 'far',\n",
       " 177: 'second',\n",
       " 178: '?',\n",
       " 179: 'good',\n",
       " 180: 'country',\n",
       " 181: 'lot',\n",
       " 182: 'were',\n",
       " 183: 'touch',\n",
       " 184: 'head',\n",
       " 185: 'sent',\n",
       " 186: 'taking',\n",
       " 187: 'issued',\n",
       " 188: 'ago',\n",
       " 189: 'few',\n",
       " 190: 'international',\n",
       " 191: 'generation',\n",
       " 192: 'his',\n",
       " 193: 'during',\n",
       " 194: 'director',\n",
       " 195: 'fact',\n",
       " 196: 'wednesday',\n",
       " 197: 'see',\n",
       " 198: 'twitter',\n",
       " 199: '...',\n",
       " 200: 'was',\n",
       " 201: 'in',\n",
       " 202: 'elections',\n",
       " 203: 'students',\n",
       " 204: 'satellite',\n",
       " 205: '2',\n",
       " 206: '$',\n",
       " 207: 'public',\n",
       " 208: 'both',\n",
       " 209: 'administration',\n",
       " 210: 'officials',\n",
       " 211: '<UNK>',\n",
       " 212: 'doctor',\n",
       " 213: 'prison',\n",
       " 214: 'children',\n",
       " 215: 'convention',\n",
       " 216: ';',\n",
       " 217: 'ministry',\n",
       " 218: 'prince',\n",
       " 219: ')',\n",
       " 220: 'despite',\n",
       " 221: 'january',\n",
       " 222: 'these',\n",
       " 223: 'money',\n",
       " 224: 'being',\n",
       " 225: 'federal',\n",
       " 226: 'him',\n",
       " 227: 'china',\n",
       " 228: 'paul',\n",
       " 229: 'move',\n",
       " 230: 'plan',\n",
       " 231: 'new',\n",
       " 232: 'half',\n",
       " 233: 'forward',\n",
       " 234: '2012',\n",
       " 235: '.',\n",
       " 236: 'set',\n",
       " 237: 'one',\n",
       " 238: 'hands',\n",
       " 239: 'court',\n",
       " 240: 'back',\n",
       " 241: 'able',\n",
       " 242: 'also',\n",
       " 243: '(',\n",
       " 244: 'how',\n",
       " 245: 'working',\n",
       " 246: 'debate',\n",
       " 247: 'point',\n",
       " 248: 'camp',\n",
       " 249: 'seen',\n",
       " 250: 'opinion',\n",
       " 251: 'business',\n",
       " 252: 'an',\n",
       " 253: 'no',\n",
       " 254: 'on',\n",
       " 255: 'made',\n",
       " 256: 'well',\n",
       " 257: 'agency',\n",
       " 258: 'sheen',\n",
       " 259: 'months',\n",
       " 260: 'may',\n",
       " 261: 'just',\n",
       " 262: 'though',\n",
       " 263: 'along',\n",
       " 264: 'great',\n",
       " 265: 'think',\n",
       " 266: 'former',\n",
       " 267: 'before',\n",
       " 268: 'own',\n",
       " 269: 'use',\n",
       " 270: 'part',\n",
       " 271: '\"',\n",
       " 272: 'syria',\n",
       " 273: 'which',\n",
       " 274: 'even',\n",
       " 275: 'off',\n",
       " 276: 'at',\n",
       " 277: 'should',\n",
       " 278: 'democratic',\n",
       " 279: 'had',\n",
       " 280: 'he',\n",
       " 281: 'have',\n",
       " 282: 'continue',\n",
       " 283: 'used',\n",
       " 284: 'side',\n",
       " 285: 'when',\n",
       " 286: 'marriage',\n",
       " 287: 'care',\n",
       " 288: 'into',\n",
       " 289: 'six',\n",
       " 290: 'released',\n",
       " 291: 'world',\n",
       " 292: 'away',\n",
       " 293: 'the',\n",
       " 294: 'after',\n",
       " 295: '--',\n",
       " 296: 'want',\n",
       " 297: 'mexico',\n",
       " 298: 'run',\n",
       " 299: 'dead',\n",
       " 300: 'minutes',\n",
       " 301: 'reported',\n",
       " 302: 'least',\n",
       " 303: 'presidential',\n",
       " 304: 'start',\n",
       " 305: 'death',\n",
       " 306: 'died',\n",
       " 307: 'florida',\n",
       " 308: 'condition',\n",
       " 309: '<S>',\n",
       " 310: 'your',\n",
       " 311: 'too',\n",
       " 312: 'not',\n",
       " 313: 'making',\n",
       " 314: 'film',\n",
       " 315: 'taken',\n",
       " 316: 'forced',\n",
       " 317: 'went',\n",
       " 318: 'month',\n",
       " 319: 'gas',\n",
       " 320: 'then',\n",
       " 321: 'hospital',\n",
       " 322: 'same',\n",
       " 323: 'family',\n",
       " 324: 'community',\n",
       " 325: 'from',\n",
       " 326: 'iraq',\n",
       " 327: 'if',\n",
       " 328: 'since',\n",
       " 329: 'russian',\n",
       " 330: 'port',\n",
       " 331: 'them',\n",
       " 332: '</S>',\n",
       " 333: 'until',\n",
       " 334: 'egypt',\n",
       " 335: 'republicans',\n",
       " 336: 'ever',\n",
       " 337: 'supreme',\n",
       " 338: 'life',\n",
       " 339: 'many',\n",
       " 340: 'old',\n",
       " 341: 'friday',\n",
       " 342: 'white',\n",
       " 343: 'key',\n",
       " 344: 'next',\n",
       " 345: 'left',\n",
       " 346: 'down',\n",
       " 347: 'law',\n",
       " 348: 'around',\n",
       " 349: 'they',\n",
       " 350: 'national',\n",
       " 351: 'across',\n",
       " 352: 'clinton',\n",
       " 353: 'washington',\n",
       " 354: 'top',\n",
       " 355: 'said',\n",
       " 356: 'police',\n",
       " 357: '!',\n",
       " 358: 'times',\n",
       " 359: 'found',\n",
       " 360: 'big',\n",
       " 361: '0',\n",
       " 362: 'week',\n",
       " 363: 'congress',\n",
       " 364: 'capital',\n",
       " 365: 'known',\n",
       " 366: 'hand',\n",
       " 367: 'student',\n",
       " 368: 'decision',\n",
       " 369: 'water',\n",
       " 370: 'judge',\n",
       " 371: 'that',\n",
       " 372: 'play',\n",
       " 373: 'could',\n",
       " 374: 'news',\n",
       " 375: 'launch',\n",
       " 376: 'hard',\n",
       " 377: \"'ll\",\n",
       " 378: 'asked',\n",
       " 379: 'flight',\n",
       " 380: 'war',\n",
       " 381: 'says',\n",
       " 382: 'party',\n",
       " 383: 'sunday',\n",
       " 384: 'america',\n",
       " 385: ',',\n",
       " 386: 'are',\n",
       " 387: 'york',\n",
       " 388: 'final',\n",
       " 389: 'other',\n",
       " 390: 'different',\n",
       " 391: 'light',\n",
       " 392: 'less',\n",
       " 393: 'south',\n",
       " 394: 'plane',\n",
       " 395: 'foreign',\n",
       " 396: 'syrian',\n",
       " 397: 'mubarak',\n",
       " 398: 'saturday',\n",
       " 399: 'likely',\n",
       " 400: 'with',\n",
       " 401: \"'\",\n",
       " 402: 'fire',\n",
       " 403: 'media',\n",
       " 404: 'i',\n",
       " 405: 'others',\n",
       " 406: 'five',\n",
       " 407: 'told',\n",
       " 408: 'each',\n",
       " 409: 'house',\n",
       " 410: \"'re\",\n",
       " 411: 'assad',\n",
       " 412: 'all',\n",
       " 413: 'page',\n",
       " 414: 'led',\n",
       " 415: 'health',\n",
       " 416: 'political',\n",
       " 417: '<PAD>',\n",
       " 418: 'iran',\n",
       " 419: 'human',\n",
       " 420: 'she',\n",
       " 421: 'first',\n",
       " 422: 'nearly',\n",
       " 423: 'city',\n",
       " 424: 'home',\n",
       " 425: 'people',\n",
       " 426: 'chinese',\n",
       " 427: 'miles',\n",
       " 428: 'screen',\n",
       " 429: 'free',\n",
       " 430: 'going',\n",
       " 431: 'much',\n",
       " 432: 'power',\n",
       " 433: 'sex',\n",
       " 434: 'dallas',\n",
       " 435: \"n't\",\n",
       " 436: 'including',\n",
       " 437: 'military',\n",
       " 438: 'united',\n",
       " 439: 'make',\n",
       " 440: 'here',\n",
       " 441: 'according',\n",
       " 442: 'win',\n",
       " 443: 'real',\n",
       " 444: 'general',\n",
       " 445: 'haiti',\n",
       " 446: 'look',\n",
       " 447: 'makes',\n",
       " 448: 'platform',\n",
       " 449: 'change',\n",
       " 450: 'office',\n",
       " 451: 'so',\n",
       " 452: 'obama',\n",
       " 453: 'report',\n",
       " 454: 'civil',\n",
       " 455: 'states',\n",
       " 456: 'up',\n",
       " 457: 'time',\n",
       " 458: 'believe',\n",
       " 459: 'government',\n",
       " 460: \"'s\",\n",
       " 461: 'u.s',\n",
       " 462: 'number',\n",
       " 463: 'muslim',\n",
       " 464: 'last',\n",
       " 465: 'investigation',\n",
       " 466: 'president',\n",
       " 467: 'system',\n",
       " 468: 'already',\n",
       " 469: 'american',\n",
       " 470: 'attention',\n",
       " 471: 'than',\n",
       " 472: 'end',\n",
       " 473: 'very',\n",
       " 474: 'kindle',\n",
       " 475: 'say',\n",
       " 476: 'took',\n",
       " 477: 'thursday',\n",
       " 478: 'or',\n",
       " 479: 'travel',\n",
       " 480: 'four',\n",
       " 481: 'several',\n",
       " 482: 'work',\n",
       " 483: '•',\n",
       " 484: 'late',\n",
       " 485: 'center',\n",
       " 486: 'case',\n",
       " 487: 'cup',\n",
       " 488: 'north',\n",
       " 489: 'vote',\n",
       " 490: 'but',\n",
       " 491: 'release',\n",
       " 492: 'john',\n",
       " 493: 'must',\n",
       " 494: 'my',\n",
       " 495: 'feel',\n",
       " 496: 'group',\n",
       " 497: 'things',\n",
       " 498: 'senate',\n",
       " 499: 'night',\n",
       " 500: 'west',\n",
       " 501: 'it',\n",
       " 502: 'who',\n",
       " 503: 'our'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 142,\n",
       " '!': 357,\n",
       " '\"': 271,\n",
       " '$': 206,\n",
       " \"'\": 401,\n",
       " \"'ll\": 377,\n",
       " \"'m\": 106,\n",
       " \"'re\": 410,\n",
       " \"'s\": 460,\n",
       " \"'ve\": 126,\n",
       " '(': 243,\n",
       " ')': 219,\n",
       " ',': 385,\n",
       " '-': 120,\n",
       " '--': 295,\n",
       " '.': 235,\n",
       " '...': 199,\n",
       " '0': 361,\n",
       " '1': 163,\n",
       " '2': 205,\n",
       " '2007': 141,\n",
       " '2008': 53,\n",
       " '2010': 47,\n",
       " '2012': 234,\n",
       " ':': 26,\n",
       " ';': 216,\n",
       " '</S>': 332,\n",
       " '<PAD>': 417,\n",
       " '<S>': 309,\n",
       " '<UNK>': 211,\n",
       " '?': 178,\n",
       " 'a': 98,\n",
       " 'able': 241,\n",
       " 'about': 16,\n",
       " 'according': 441,\n",
       " 'across': 351,\n",
       " 'act': 29,\n",
       " 'administration': 209,\n",
       " 'after': 294,\n",
       " 'against': 48,\n",
       " 'agency': 257,\n",
       " 'ago': 188,\n",
       " 'al': 10,\n",
       " 'all': 412,\n",
       " 'along': 263,\n",
       " 'already': 468,\n",
       " 'also': 242,\n",
       " 'amazon': 42,\n",
       " 'america': 384,\n",
       " 'american': 469,\n",
       " 'among': 160,\n",
       " 'an': 252,\n",
       " 'and': 92,\n",
       " 'another': 102,\n",
       " 'anti': 156,\n",
       " 'any': 108,\n",
       " 'are': 386,\n",
       " 'areas': 60,\n",
       " 'around': 348,\n",
       " 'as': 23,\n",
       " 'asked': 378,\n",
       " 'assad': 411,\n",
       " 'at': 276,\n",
       " 'attention': 470,\n",
       " 'au': 35,\n",
       " 'authorities': 101,\n",
       " 'away': 292,\n",
       " 'back': 240,\n",
       " 'based': 123,\n",
       " 'be': 25,\n",
       " 'because': 12,\n",
       " 'been': 75,\n",
       " 'before': 267,\n",
       " 'being': 224,\n",
       " 'believe': 458,\n",
       " 'best': 136,\n",
       " 'better': 140,\n",
       " 'between': 110,\n",
       " 'big': 360,\n",
       " 'book': 166,\n",
       " 'both': 208,\n",
       " 'business': 251,\n",
       " 'but': 490,\n",
       " 'by': 37,\n",
       " 'called': 20,\n",
       " 'camp': 248,\n",
       " 'campaign': 88,\n",
       " 'camps': 66,\n",
       " 'can': 34,\n",
       " 'capital': 364,\n",
       " 'care': 287,\n",
       " 'case': 486,\n",
       " 'center': 485,\n",
       " 'change': 449,\n",
       " 'cherokee': 111,\n",
       " 'children': 214,\n",
       " 'china': 227,\n",
       " 'chinese': 426,\n",
       " 'city': 423,\n",
       " 'civil': 454,\n",
       " 'clinton': 352,\n",
       " 'cnn': 74,\n",
       " 'come': 143,\n",
       " 'community': 324,\n",
       " 'condition': 308,\n",
       " 'congress': 363,\n",
       " 'constitution': 135,\n",
       " 'continue': 282,\n",
       " 'contributed': 167,\n",
       " 'control': 82,\n",
       " 'convention': 215,\n",
       " 'could': 373,\n",
       " 'country': 180,\n",
       " 'court': 239,\n",
       " 'cup': 487,\n",
       " 'dallas': 434,\n",
       " 'day': 55,\n",
       " 'days': 24,\n",
       " 'dead': 299,\n",
       " 'deal': 63,\n",
       " 'death': 305,\n",
       " 'debate': 246,\n",
       " 'decision': 368,\n",
       " 'defense': 18,\n",
       " 'democratic': 278,\n",
       " 'department': 51,\n",
       " 'despite': 220,\n",
       " 'did': 33,\n",
       " 'died': 306,\n",
       " 'different': 390,\n",
       " 'director': 194,\n",
       " 'do': 129,\n",
       " 'doctor': 212,\n",
       " 'does': 61,\n",
       " 'down': 346,\n",
       " 'during': 193,\n",
       " 'e': 27,\n",
       " 'each': 408,\n",
       " 'earlier': 122,\n",
       " 'early': 170,\n",
       " 'egypt': 334,\n",
       " 'election': 14,\n",
       " 'elections': 202,\n",
       " 'end': 472,\n",
       " 'equality': 6,\n",
       " 'even': 274,\n",
       " 'event': 80,\n",
       " 'ever': 336,\n",
       " 'expected': 31,\n",
       " 'fact': 195,\n",
       " 'family': 323,\n",
       " 'far': 176,\n",
       " 'federal': 225,\n",
       " 'feel': 495,\n",
       " 'few': 189,\n",
       " 'film': 314,\n",
       " 'final': 388,\n",
       " 'fire': 402,\n",
       " 'first': 421,\n",
       " 'five': 406,\n",
       " 'flight': 379,\n",
       " 'florida': 307,\n",
       " 'for': 77,\n",
       " 'forced': 316,\n",
       " 'forces': 65,\n",
       " 'foreign': 395,\n",
       " 'former': 266,\n",
       " 'forward': 233,\n",
       " 'found': 359,\n",
       " 'four': 480,\n",
       " 'free': 429,\n",
       " 'friday': 341,\n",
       " 'from': 325,\n",
       " 'gas': 319,\n",
       " 'general': 444,\n",
       " 'generation': 191,\n",
       " 'get': 131,\n",
       " 'gift': 70,\n",
       " 'go': 11,\n",
       " 'going': 430,\n",
       " 'good': 179,\n",
       " 'got': 9,\n",
       " 'government': 459,\n",
       " 'great': 264,\n",
       " 'group': 496,\n",
       " 'had': 279,\n",
       " 'haiti': 445,\n",
       " 'half': 232,\n",
       " 'hand': 366,\n",
       " 'hands': 238,\n",
       " 'hard': 376,\n",
       " 'has': 44,\n",
       " 'have': 281,\n",
       " 'he': 280,\n",
       " 'head': 184,\n",
       " 'health': 415,\n",
       " 'heard': 86,\n",
       " 'held': 54,\n",
       " 'help': 118,\n",
       " 'her': 93,\n",
       " 'here': 440,\n",
       " 'high': 95,\n",
       " 'him': 226,\n",
       " 'his': 192,\n",
       " 'history': 125,\n",
       " 'hit': 117,\n",
       " 'home': 424,\n",
       " 'hospital': 321,\n",
       " 'house': 409,\n",
       " 'how': 244,\n",
       " 'human': 419,\n",
       " 'hurricane': 155,\n",
       " 'i': 404,\n",
       " 'if': 327,\n",
       " 'in': 201,\n",
       " 'including': 436,\n",
       " 'international': 190,\n",
       " 'into': 288,\n",
       " 'investigation': 465,\n",
       " 'iran': 418,\n",
       " 'iraq': 326,\n",
       " 'is': 149,\n",
       " 'issue': 138,\n",
       " 'issued': 187,\n",
       " 'issues': 28,\n",
       " 'istanbul': 144,\n",
       " 'it': 501,\n",
       " 'its': 99,\n",
       " 'january': 221,\n",
       " 'john': 492,\n",
       " 'judge': 370,\n",
       " 'just': 261,\n",
       " 'key': 343,\n",
       " 'killed': 145,\n",
       " 'kindle': 474,\n",
       " 'know': 78,\n",
       " 'known': 365,\n",
       " 'last': 464,\n",
       " 'late': 484,\n",
       " 'launch': 375,\n",
       " 'law': 347,\n",
       " 'leader': 150,\n",
       " 'least': 302,\n",
       " 'led': 414,\n",
       " 'left': 345,\n",
       " 'less': 392,\n",
       " 'life': 338,\n",
       " 'light': 391,\n",
       " 'like': 3,\n",
       " 'likely': 399,\n",
       " 'local': 134,\n",
       " 'long': 130,\n",
       " 'look': 446,\n",
       " 'lot': 181,\n",
       " 'made': 255,\n",
       " 'major': 100,\n",
       " 'majority': 112,\n",
       " 'make': 439,\n",
       " 'makes': 447,\n",
       " 'making': 313,\n",
       " 'man': 161,\n",
       " 'many': 339,\n",
       " 'marriage': 286,\n",
       " 'may': 260,\n",
       " 'mccain': 91,\n",
       " 'me': 87,\n",
       " 'means': 94,\n",
       " 'media': 403,\n",
       " 'medical': 153,\n",
       " 'members': 71,\n",
       " 'men': 159,\n",
       " 'mexico': 297,\n",
       " 'miles': 427,\n",
       " 'military': 437,\n",
       " 'million': 58,\n",
       " 'ministry': 217,\n",
       " 'minutes': 300,\n",
       " 'monday': 115,\n",
       " 'money': 223,\n",
       " 'month': 318,\n",
       " 'months': 259,\n",
       " 'more': 152,\n",
       " 'most': 8,\n",
       " 'move': 229,\n",
       " 'movement': 38,\n",
       " 'mubarak': 397,\n",
       " 'much': 431,\n",
       " 'muslim': 463,\n",
       " 'must': 493,\n",
       " 'my': 494,\n",
       " \"n't\": 435,\n",
       " 'nation': 128,\n",
       " 'national': 350,\n",
       " 'nearly': 422,\n",
       " 'never': 62,\n",
       " 'new': 231,\n",
       " 'news': 374,\n",
       " 'next': 344,\n",
       " 'night': 499,\n",
       " 'no': 253,\n",
       " 'north': 488,\n",
       " 'not': 312,\n",
       " 'now': 154,\n",
       " 'number': 462,\n",
       " 'obama': 452,\n",
       " 'of': 57,\n",
       " 'off': 275,\n",
       " 'office': 450,\n",
       " 'official': 45,\n",
       " 'officials': 210,\n",
       " 'old': 340,\n",
       " 'on': 254,\n",
       " 'one': 237,\n",
       " 'only': 113,\n",
       " 'open': 7,\n",
       " 'opinion': 250,\n",
       " 'or': 478,\n",
       " 'other': 389,\n",
       " 'others': 405,\n",
       " 'our': 503,\n",
       " 'out': 164,\n",
       " 'outside': 114,\n",
       " 'over': 81,\n",
       " 'own': 268,\n",
       " 'page': 413,\n",
       " 'paperwhite': 173,\n",
       " 'part': 270,\n",
       " 'party': 382,\n",
       " 'past': 46,\n",
       " 'paul': 228,\n",
       " 'people': 425,\n",
       " 'percent': 84,\n",
       " 'photos': 90,\n",
       " 'place': 36,\n",
       " 'plan': 230,\n",
       " 'plane': 394,\n",
       " 'platform': 448,\n",
       " 'play': 372,\n",
       " 'point': 247,\n",
       " 'police': 356,\n",
       " 'policy': 32,\n",
       " 'political': 416,\n",
       " 'port': 330,\n",
       " 'power': 432,\n",
       " 'president': 466,\n",
       " 'presidential': 303,\n",
       " 'prince': 218,\n",
       " 'prison': 213,\n",
       " 'process': 13,\n",
       " 'public': 207,\n",
       " 'put': 172,\n",
       " 'race': 83,\n",
       " 'real': 443,\n",
       " 'really': 157,\n",
       " 'release': 491,\n",
       " 'released': 290,\n",
       " 'report': 453,\n",
       " 'reported': 301,\n",
       " 'reports': 73,\n",
       " 'republican': 104,\n",
       " 'republicans': 335,\n",
       " 'right': 68,\n",
       " 'rights': 19,\n",
       " 'run': 298,\n",
       " 'russian': 329,\n",
       " 'said': 355,\n",
       " 'same': 322,\n",
       " 'satellite': 204,\n",
       " 'saturday': 398,\n",
       " 'say': 475,\n",
       " 'saying': 174,\n",
       " 'says': 381,\n",
       " 'school': 132,\n",
       " 'screen': 428,\n",
       " 'second': 177,\n",
       " 'security': 147,\n",
       " 'see': 197,\n",
       " 'seen': 249,\n",
       " 'sen': 49,\n",
       " 'senate': 498,\n",
       " 'sent': 185,\n",
       " 'set': 236,\n",
       " 'several': 481,\n",
       " 'sex': 433,\n",
       " 'she': 420,\n",
       " 'sheen': 258,\n",
       " 'should': 277,\n",
       " 'show': 21,\n",
       " 'side': 284,\n",
       " 'since': 328,\n",
       " 'six': 289,\n",
       " 'small': 4,\n",
       " 'so': 451,\n",
       " 'some': 109,\n",
       " 'south': 393,\n",
       " 'spokesman': 165,\n",
       " 'start': 304,\n",
       " 'state': 15,\n",
       " 'statement': 146,\n",
       " 'states': 455,\n",
       " 'sterling': 124,\n",
       " 'stewart': 59,\n",
       " 'still': 116,\n",
       " 'storm': 107,\n",
       " 'student': 367,\n",
       " 'students': 203,\n",
       " 'such': 64,\n",
       " 'sunday': 383,\n",
       " 'support': 67,\n",
       " 'supreme': 337,\n",
       " 'sure': 85,\n",
       " 'syria': 272,\n",
       " 'syrian': 396,\n",
       " 'system': 467,\n",
       " 'take': 171,\n",
       " 'taken': 315,\n",
       " 'taking': 186,\n",
       " 'team': 105,\n",
       " 'technology': 17,\n",
       " 'terrorist': 89,\n",
       " 'than': 471,\n",
       " 'that': 371,\n",
       " 'the': 293,\n",
       " 'their': 169,\n",
       " 'them': 331,\n",
       " 'then': 320,\n",
       " 'there': 97,\n",
       " 'these': 222,\n",
       " 'they': 349,\n",
       " 'things': 497,\n",
       " 'think': 265,\n",
       " 'this': 103,\n",
       " 'thomas': 52,\n",
       " 'those': 50,\n",
       " 'though': 262,\n",
       " 'three': 148,\n",
       " 'through': 72,\n",
       " 'thursday': 477,\n",
       " 'time': 457,\n",
       " 'times': 358,\n",
       " 'to': 39,\n",
       " 'today': 22,\n",
       " 'told': 407,\n",
       " 'too': 311,\n",
       " 'took': 476,\n",
       " 'top': 354,\n",
       " 'touch': 183,\n",
       " 'travel': 479,\n",
       " 'trying': 133,\n",
       " 'tuesday': 158,\n",
       " 'twitter': 198,\n",
       " 'two': 43,\n",
       " 'u.s': 461,\n",
       " 'under': 30,\n",
       " 'united': 438,\n",
       " 'university': 56,\n",
       " 'until': 333,\n",
       " 'up': 456,\n",
       " 'us': 79,\n",
       " 'use': 269,\n",
       " 'used': 283,\n",
       " 'va': 96,\n",
       " 'very': 473,\n",
       " 'vote': 489,\n",
       " 'want': 296,\n",
       " 'war': 380,\n",
       " 'was': 200,\n",
       " 'washington': 353,\n",
       " 'water': 369,\n",
       " 'way': 127,\n",
       " 'we': 139,\n",
       " 'wednesday': 196,\n",
       " 'week': 362,\n",
       " 'well': 256,\n",
       " 'went': 317,\n",
       " 'were': 182,\n",
       " 'west': 500,\n",
       " 'what': 76,\n",
       " 'when': 285,\n",
       " 'where': 162,\n",
       " 'which': 273,\n",
       " 'while': 121,\n",
       " 'white': 342,\n",
       " 'who': 502,\n",
       " 'will': 41,\n",
       " 'win': 442,\n",
       " 'with': 400,\n",
       " 'without': 137,\n",
       " 'woman': 168,\n",
       " 'women': 40,\n",
       " 'work': 482,\n",
       " 'working': 245,\n",
       " 'world': 291,\n",
       " 'would': 69,\n",
       " 'wrote': 151,\n",
       " 'year': 5,\n",
       " 'years': 175,\n",
       " 'york': 387,\n",
       " 'you': 119,\n",
       " 'your': 310,\n",
       " '•': 483}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_to_tokens(token_list,word2idx):\n",
    "    out = []\n",
    "    oov2idx = dict()\n",
    "    oov_idx = 0\n",
    "    for token in token_list:\n",
    "        word = token.text\n",
    "        try:\n",
    "            out.append(word2idx[word])\n",
    "        except KeyError:\n",
    "            if word not in oov2idx:\n",
    "                oov_idx+=1\n",
    "                oov2idx[word]=vocab_size+oov_idx\n",
    "            out.append(oov2idx[word])\n",
    "    return out, oov2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "out, oov2idx = nlp_to_tokens(list(body_tokens),word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[23,\n",
       " 31,\n",
       " 61,\n",
       " 36,\n",
       " 57,\n",
       " 10,\n",
       " 450,\n",
       " 501,\n",
       " 12,\n",
       " 119,\n",
       " 261,\n",
       " 73,\n",
       " 10,\n",
       " 390,\n",
       " 30,\n",
       " 221,\n",
       " 4,\n",
       " 220,\n",
       " 8,\n",
       " 70,\n",
       " 502,\n",
       " 9,\n",
       " 70,\n",
       " 503,\n",
       " 18,\n",
       " 10,\n",
       " 504,\n",
       " 7,\n",
       " 505,\n",
       " 10,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 9,\n",
       " 316,\n",
       " 4,\n",
       " 509,\n",
       " 8,\n",
       " 4,\n",
       " 510,\n",
       " 18,\n",
       " 4,\n",
       " 427,\n",
       " 5,\n",
       " 44,\n",
       " 250,\n",
       " 297,\n",
       " 7,\n",
       " 45,\n",
       " 8,\n",
       " 99,\n",
       " 501,\n",
       " 5,\n",
       " 46,\n",
       " 511,\n",
       " 512,\n",
       " 94,\n",
       " 6,\n",
       " 10,\n",
       " 513,\n",
       " 514,\n",
       " 390,\n",
       " 30,\n",
       " 474,\n",
       " 4,\n",
       " 254,\n",
       " 7,\n",
       " 337,\n",
       " 22,\n",
       " 70,\n",
       " 515,\n",
       " 516,\n",
       " 6,\n",
       " 128,\n",
       " 7,\n",
       " 517,\n",
       " 518,\n",
       " 9,\n",
       " 519,\n",
       " 520,\n",
       " 12,\n",
       " 62,\n",
       " 521,\n",
       " 482,\n",
       " 6,\n",
       " 11,\n",
       " 522,\n",
       " 523,\n",
       " 5,\n",
       " 11,\n",
       " 64,\n",
       " 524,\n",
       " 4,\n",
       " 164,\n",
       " 525,\n",
       " 6,\n",
       " 4,\n",
       " 478,\n",
       " 305,\n",
       " 9,\n",
       " 4,\n",
       " 59,\n",
       " 337,\n",
       " 25,\n",
       " 492,\n",
       " 501,\n",
       " 73,\n",
       " 10,\n",
       " 390,\n",
       " 526,\n",
       " 527,\n",
       " 7,\n",
       " 528,\n",
       " 10,\n",
       " 488,\n",
       " 508,\n",
       " 5,\n",
       " 323,\n",
       " 13,\n",
       " 522,\n",
       " 390,\n",
       " 16,\n",
       " 4,\n",
       " 383,\n",
       " 390,\n",
       " 7,\n",
       " 529,\n",
       " 14,\n",
       " 220,\n",
       " 5,\n",
       " 37,\n",
       " 530,\n",
       " 10,\n",
       " 531,\n",
       " 532,\n",
       " 5,\n",
       " 12,\n",
       " 4,\n",
       " 266,\n",
       " 533,\n",
       " 6,\n",
       " 184,\n",
       " 390,\n",
       " 534,\n",
       " 7,\n",
       " 129,\n",
       " 4,\n",
       " 204,\n",
       " 14,\n",
       " 10,\n",
       " 471,\n",
       " 13,\n",
       " 116,\n",
       " 7,\n",
       " 535,\n",
       " 118,\n",
       " 26,\n",
       " 536,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 537,\n",
       " 94,\n",
       " 325,\n",
       " 17,\n",
       " 211,\n",
       " 6,\n",
       " 4,\n",
       " 215,\n",
       " 109,\n",
       " 538,\n",
       " 195,\n",
       " 4,\n",
       " 539,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 298,\n",
       " 5,\n",
       " 323,\n",
       " 6,\n",
       " 543,\n",
       " 75,\n",
       " 537,\n",
       " 36,\n",
       " 544,\n",
       " 545,\n",
       " 12,\n",
       " 35,\n",
       " 182,\n",
       " 133,\n",
       " 546,\n",
       " 30,\n",
       " 57,\n",
       " 547,\n",
       " 5,\n",
       " 4,\n",
       " 548,\n",
       " 15,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 182,\n",
       " 16,\n",
       " 35,\n",
       " 182,\n",
       " 37,\n",
       " 549,\n",
       " 69,\n",
       " 342,\n",
       " 107,\n",
       " 5,\n",
       " 130,\n",
       " 9,\n",
       " 54,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 550,\n",
       " 265,\n",
       " 551,\n",
       " 552,\n",
       " 7,\n",
       " 115,\n",
       " 10,\n",
       " 553,\n",
       " 6,\n",
       " 547,\n",
       " 546,\n",
       " 5,\n",
       " 554,\n",
       " 6,\n",
       " 49,\n",
       " 555,\n",
       " 75,\n",
       " 39,\n",
       " 6,\n",
       " 265,\n",
       " 556,\n",
       " 12,\n",
       " 4,\n",
       " 55,\n",
       " 5,\n",
       " 9,\n",
       " 41,\n",
       " 474,\n",
       " 7,\n",
       " 154,\n",
       " 7,\n",
       " 141,\n",
       " 15,\n",
       " 557,\n",
       " 558,\n",
       " 133,\n",
       " 64,\n",
       " 41,\n",
       " 559,\n",
       " 9,\n",
       " 560,\n",
       " 175,\n",
       " 458,\n",
       " 29,\n",
       " 144,\n",
       " 561,\n",
       " 5,\n",
       " 562,\n",
       " 8,\n",
       " 14,\n",
       " 563,\n",
       " 38,\n",
       " 564,\n",
       " 25,\n",
       " 87,\n",
       " 565,\n",
       " 33,\n",
       " 5,\n",
       " 34,\n",
       " 32,\n",
       " 356,\n",
       " 148,\n",
       " 566,\n",
       " 30,\n",
       " 567,\n",
       " 14,\n",
       " 568,\n",
       " 117,\n",
       " 38,\n",
       " 168,\n",
       " 17,\n",
       " 4,\n",
       " 569,\n",
       " 5,\n",
       " 126,\n",
       " 540,\n",
       " 570,\n",
       " 571,\n",
       " 4,\n",
       " 116,\n",
       " 7,\n",
       " 535,\n",
       " 6,\n",
       " 10,\n",
       " 572,\n",
       " 9,\n",
       " 573,\n",
       " 433,\n",
       " 16,\n",
       " 574,\n",
       " 377,\n",
       " 27,\n",
       " 10,\n",
       " 471,\n",
       " 13,\n",
       " 575,\n",
       " 7,\n",
       " 576,\n",
       " 72,\n",
       " 568,\n",
       " 117,\n",
       " 5,\n",
       " 12,\n",
       " 436,\n",
       " 6,\n",
       " 577,\n",
       " 578,\n",
       " 579,\n",
       " 402,\n",
       " 15,\n",
       " 580,\n",
       " 581,\n",
       " 12,\n",
       " 582,\n",
       " 124,\n",
       " 5,\n",
       " 25,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 583,\n",
       " 70,\n",
       " 584,\n",
       " 585,\n",
       " 6,\n",
       " 518,\n",
       " 9,\n",
       " 520,\n",
       " 13,\n",
       " 522,\n",
       " 390,\n",
       " 16,\n",
       " 586,\n",
       " 8,\n",
       " 587,\n",
       " 5,\n",
       " 588,\n",
       " 589,\n",
       " 214,\n",
       " 6,\n",
       " 33,\n",
       " 4,\n",
       " 590,\n",
       " 148,\n",
       " 6,\n",
       " 64,\n",
       " 43,\n",
       " 129,\n",
       " 63,\n",
       " 400,\n",
       " 591,\n",
       " 8,\n",
       " 4,\n",
       " 592,\n",
       " 5,\n",
       " 64,\n",
       " 38,\n",
       " 593,\n",
       " 6,\n",
       " 594,\n",
       " 9,\n",
       " 595,\n",
       " 5,\n",
       " 64,\n",
       " 43,\n",
       " 26,\n",
       " 332,\n",
       " 7,\n",
       " 596,\n",
       " 370,\n",
       " 6,\n",
       " 597,\n",
       " 225,\n",
       " 393,\n",
       " 9,\n",
       " 598,\n",
       " 182,\n",
       " 5,\n",
       " 599,\n",
       " 600,\n",
       " 183,\n",
       " 14,\n",
       " 601,\n",
       " 38,\n",
       " 602,\n",
       " 603,\n",
       " 15,\n",
       " 580,\n",
       " 6,\n",
       " 9,\n",
       " 44,\n",
       " 36,\n",
       " 366,\n",
       " 97,\n",
       " 139,\n",
       " 10,\n",
       " 604,\n",
       " 5,\n",
       " 84,\n",
       " 100,\n",
       " 8,\n",
       " 4,\n",
       " 225,\n",
       " 6,\n",
       " 601,\n",
       " 399,\n",
       " 14,\n",
       " 605,\n",
       " 118,\n",
       " 39,\n",
       " 606,\n",
       " 22,\n",
       " 265,\n",
       " 13,\n",
       " 191,\n",
       " 359,\n",
       " 5,\n",
       " 9,\n",
       " 12,\n",
       " 607,\n",
       " 6,\n",
       " 601,\n",
       " 36,\n",
       " 608,\n",
       " 609,\n",
       " 7,\n",
       " 610,\n",
       " 611,\n",
       " 13,\n",
       " 11,\n",
       " 612,\n",
       " 11,\n",
       " 613,\n",
       " 6,\n",
       " 7,\n",
       " 614,\n",
       " 615,\n",
       " 18,\n",
       " 265,\n",
       " 13,\n",
       " 191,\n",
       " 616,\n",
       " 9,\n",
       " 7,\n",
       " 617,\n",
       " 402,\n",
       " 15,\n",
       " 580,\n",
       " 618,\n",
       " 12,\n",
       " 4,\n",
       " 436,\n",
       " 148,\n",
       " 5,\n",
       " 34,\n",
       " 19,\n",
       " 16,\n",
       " 619,\n",
       " 14,\n",
       " 175,\n",
       " 49,\n",
       " 603,\n",
       " 15,\n",
       " 580,\n",
       " 601,\n",
       " 453,\n",
       " 7,\n",
       " 173,\n",
       " 4,\n",
       " 620,\n",
       " 167,\n",
       " 4,\n",
       " 621,\n",
       " 9,\n",
       " 4,\n",
       " 152,\n",
       " 105,\n",
       " 402,\n",
       " 15,\n",
       " 580,\n",
       " 622,\n",
       " 38,\n",
       " 39,\n",
       " 623,\n",
       " 195,\n",
       " 5,\n",
       " 171,\n",
       " 624,\n",
       " 54,\n",
       " 540,\n",
       " 40,\n",
       " 10,\n",
       " 625,\n",
       " 8,\n",
       " 252,\n",
       " 9,\n",
       " 626,\n",
       " 123,\n",
       " 4,\n",
       " 356,\n",
       " 86,\n",
       " 282,\n",
       " 370,\n",
       " 6,\n",
       " 44,\n",
       " 36,\n",
       " 366,\n",
       " 601,\n",
       " 77,\n",
       " 575,\n",
       " 7,\n",
       " 576,\n",
       " 62,\n",
       " 220,\n",
       " 6,\n",
       " 627,\n",
       " 63,\n",
       " 9,\n",
       " 316,\n",
       " 4,\n",
       " 628,\n",
       " 8,\n",
       " 119,\n",
       " 254,\n",
       " 5,\n",
       " 120,\n",
       " 38,\n",
       " 629,\n",
       " 63,\n",
       " 630,\n",
       " 6,\n",
       " 12,\n",
       " 4,\n",
       " 631,\n",
       " 632,\n",
       " 6,\n",
       " 27,\n",
       " 4,\n",
       " 633,\n",
       " 15,\n",
       " 634,\n",
       " 635,\n",
       " 9,\n",
       " 12,\n",
       " 4,\n",
       " 636,\n",
       " 8,\n",
       " 119,\n",
       " 637,\n",
       " 5,\n",
       " 64,\n",
       " 38,\n",
       " 638,\n",
       " 62,\n",
       " 639,\n",
       " 9,\n",
       " 640,\n",
       " 62,\n",
       " 218,\n",
       " 641,\n",
       " 6,\n",
       " 34,\n",
       " 19,\n",
       " 16,\n",
       " 642,\n",
       " 14,\n",
       " 44,\n",
       " 528,\n",
       " 643,\n",
       " 7,\n",
       " 644,\n",
       " 9,\n",
       " 645,\n",
       " 10,\n",
       " 59,\n",
       " 646,\n",
       " 46,\n",
       " 647,\n",
       " 265,\n",
       " 13,\n",
       " 568,\n",
       " 191,\n",
       " 5,\n",
       " 61,\n",
       " 16,\n",
       " 648,\n",
       " 649,\n",
       " 18,\n",
       " 67,\n",
       " 32,\n",
       " 390,\n",
       " 60,\n",
       " 650,\n",
       " 6,\n",
       " 9,\n",
       " 133,\n",
       " 19,\n",
       " 651,\n",
       " 7,\n",
       " 652,\n",
       " 4,\n",
       " 646,\n",
       " 46,\n",
       " 4,\n",
       " 116,\n",
       " 7,\n",
       " 535,\n",
       " 5,\n",
       " 25,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 653,\n",
       " 537,\n",
       " 6,\n",
       " 4,\n",
       " 654,\n",
       " 8,\n",
       " 580,\n",
       " 16,\n",
       " 27,\n",
       " 10,\n",
       " 655,\n",
       " 5,\n",
       " 37,\n",
       " 196,\n",
       " 6,\n",
       " 22,\n",
       " 119,\n",
       " 656,\n",
       " 657,\n",
       " 6,\n",
       " 44,\n",
       " 43,\n",
       " 658,\n",
       " 32,\n",
       " 333,\n",
       " 15,\n",
       " 659,\n",
       " 660,\n",
       " 18,\n",
       " 661,\n",
       " 7,\n",
       " 297,\n",
       " 5,\n",
       " 4,\n",
       " 662,\n",
       " 663,\n",
       " 12,\n",
       " 32,\n",
       " 664,\n",
       " 38,\n",
       " 665,\n",
       " 99,\n",
       " 8,\n",
       " 666,\n",
       " 667,\n",
       " 5]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(body)\n",
    "lst = list(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(set(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "after",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-93582cfc1c0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: after",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-93582cfc1c0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moov_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: after"
     ]
    }
   ],
   "source": [
    "out= []\n",
    "oov_dict = dict()\n",
    "for x in words:\n",
    "    try:\n",
    "        y = word2idx[x]\n",
    "        out.append(y)\n",
    "    except KeyError:\n",
    "        oov_dict[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "352"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['small']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.most_common(300)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = list(np.arange(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "i = 0\n",
    "for file_name in file_list[0:200]:\n",
    "    with open(file_name) as f:\n",
    "        text = f.read()\n",
    "        text = text.lower()\n",
    "        text = text.replace('\\n\\n',' ')\n",
    "        text = text.replace('(cnn)','')\n",
    "        text = text.split(\"@highlight\")\n",
    "        body = text[0]\n",
    "        doc = list(nlp(body))\n",
    "        word_list.extend([x.text for x in doc])\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = c + Counter(['a','b','a','b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.most_common(100)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14454"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(word_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "a = Variable(torch.LongTensor(np.arange(40).reshape(4,10)))\n",
    "emb = nn.Embedding(40,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LSTM in module torch.nn.modules.rnn:\n",
      "\n",
      "class LSTM(RNNBase)\n",
      " |  Applies a multi-layer long short-term memory (LSTM) RNN to an input\n",
      " |  sequence.\n",
      " |  \n",
      " |  \n",
      " |  For each element in the input sequence, each layer computes the following\n",
      " |  function:\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |          \\begin{array}{ll}\n",
      " |          i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
      " |          f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
      " |          g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n",
      " |          o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n",
      " |          c_t = f_t c_{(t-1)} + i_t g_t \\\\\n",
      " |          h_t = o_t \\tanh(c_t)\n",
      " |          \\end{array}\n",
      " |  \n",
      " |  where :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the cell\n",
      " |  state at time `t`, :math:`x_t` is the input at time `t`, :math:`h_{(t-1)}`\n",
      " |  is the hidden state of the previous layer at time `t-1` or the initial hidden\n",
      " |  state at time `0`, and :math:`i_t`, :math:`f_t`, :math:`g_t`,\n",
      " |  :math:`o_t` are the input, forget, cell, and output gates, respectively.\n",
      " |  :math:`\\sigma` is the sigmoid function.\n",
      " |  \n",
      " |  Args:\n",
      " |      input_size: The number of expected features in the input `x`\n",
      " |      hidden_size: The number of features in the hidden state `h`\n",
      " |      num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``\n",
      " |          would mean stacking two LSTMs together to form a `stacked LSTM`,\n",
      " |          with the second LSTM taking in outputs of the first LSTM and\n",
      " |          computing the final results. Default: 1\n",
      " |      bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.\n",
      " |          Default: ``True``\n",
      " |      batch_first: If ``True``, then the input and output tensors are provided\n",
      " |          as (batch, seq, feature)\n",
      " |      dropout: If non-zero, introduces a `Dropout` layer on the outputs of each\n",
      " |          LSTM layer except the last layer, with dropout probability equal to\n",
      " |          :attr:`dropout`. Default: 0\n",
      " |      bidirectional: If ``True``, becomes a bidirectional LSTM. Default: ``False``\n",
      " |  \n",
      " |  Inputs: input, (h_0, c_0)\n",
      " |      - **input** of shape `(seq_len, batch, input_size)`: tensor containing the features\n",
      " |        of the input sequence.\n",
      " |        The input can also be a packed variable length sequence.\n",
      " |        See :func:`torch.nn.utils.rnn.pack_padded_sequence` or\n",
      " |        :func:`torch.nn.utils.rnn.pack_sequence` for details.\n",
      " |      - **h_0** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\n",
      " |        containing the initial hidden state for each element in the batch.\n",
      " |      - **c_0** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\n",
      " |        containing the initial cell state for each element in the batch.\n",
      " |  \n",
      " |        If `(h_0, c_0)` is not provided, both **h_0** and **c_0** default to zero.\n",
      " |  \n",
      " |  \n",
      " |  Outputs: output, (h_n, c_n)\n",
      " |      - **output** of shape `(seq_len, batch, hidden_size * num_directions)`: tensor\n",
      " |        containing the output features `(h_t)` from the last layer of the LSTM,\n",
      " |        for each t. If a :class:`torch.nn.utils.rnn.PackedSequence` has been\n",
      " |        given as the input, the output will also be a packed sequence.\n",
      " |      - **h_n** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\n",
      " |        containing the hidden state for `t = seq_len`\n",
      " |      - **c_n** (num_layers * num_directions, batch, hidden_size): tensor\n",
      " |        containing the cell state for `t = seq_len`\n",
      " |  \n",
      " |  Attributes:\n",
      " |      weight_ih_l[k] : the learnable input-hidden weights of the :math:`\\text{k}^{th}` layer\n",
      " |          `(W_ii|W_if|W_ig|W_io)`, of shape `(4*hidden_size x input_size)`\n",
      " |      weight_hh_l[k] : the learnable hidden-hidden weights of the :math:`\\text{k}^{th}` layer\n",
      " |          `(W_hi|W_hf|W_hg|W_ho)`, of shape `(4*hidden_size x hidden_size)`\n",
      " |      bias_ih_l[k] : the learnable input-hidden bias of the :math:`\\text{k}^{th}` layer\n",
      " |          `(b_ii|b_if|b_ig|b_io)`, of shape `(4*hidden_size)`\n",
      " |      bias_hh_l[k] : the learnable hidden-hidden bias of the :math:`\\text{k}^{th}` layer\n",
      " |          `(b_hi|b_hf|b_hg|b_ho)`, of shape `(4*hidden_size)`\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> rnn = nn.LSTM(10, 20, 2)\n",
      " |      >>> input = torch.randn(5, 3, 10)\n",
      " |      >>> h0 = torch.randn(2, 3, 20)\n",
      " |      >>> c0 = torch.randn(2, 3, 20)\n",
      " |      >>> output, hn = rnn(input, (h0, c0))\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LSTM\n",
      " |      RNNBase\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from RNNBase:\n",
      " |  \n",
      " |  __setstate__(self, d)\n",
      " |  \n",
      " |  check_forward_args(self, input, hidden, batch_sizes)\n",
      " |  \n",
      " |  extra_repr(self)\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should reimplement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  flatten_parameters(self)\n",
      " |      Resets parameter data pointer so that they can use faster code paths.\n",
      " |      \n",
      " |      Right now, this works only if the module is on the GPU and cuDNN is enabled.\n",
      " |      Otherwise, it's a no-op.\n",
      " |  \n",
      " |  forward(self, input, hx=None)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  reset_parameters(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from RNNBase:\n",
      " |  \n",
      " |  all_weights\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__(self, *input, **kwargs)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      __dir__() -> list\n",
      " |      default dir() implementation\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  add_module(self, name, module)\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          parameter (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn)\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`torch-nn-init`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> def init_weights(m):\n",
      " |                  print(m)\n",
      " |                  if type(m) == nn.Linear:\n",
      " |                      m.weight.data.fill_(1.0)\n",
      " |                      print(m.weight)\n",
      " |      \n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  children(self)\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self)\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device=None)\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self)\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self)\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |  \n",
      " |  float(self)\n",
      " |      Casts all floating point parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self)\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict, strict=True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |  \n",
      " |  modules(self)\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          )\n",
      " |          1 -> Linear (2 -> 2)\n",
      " |  \n",
      " |  named_children(self)\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo=None, prefix='')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential (\n",
      " |            (0): Linear (2 -> 2)\n",
      " |            (1): Linear (2 -> 2)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear (2 -> 2))\n",
      " |  \n",
      " |  named_parameters(self, memo=None, prefix='')\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self)\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param.data), param.size())\n",
      " |          <class 'torch.FloatTensor'> (20L,)\n",
      " |          <class 'torch.FloatTensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook)\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> Tensor or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` may be tuples if the\n",
      " |      module has multiple inputs or outputs. The hook should not modify its\n",
      " |      arguments, but it can optionally return a new gradient with respect to\n",
      " |      input that will be used in place of :attr:`grad_input` in subsequent\n",
      " |      computations.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name, tensor)\n",
      " |      Adds a persistent buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the persistent state.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook)\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None\n",
      " |      \n",
      " |      The hook should not modify the input or output.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook)\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None\n",
      " |      \n",
      " |      The hook should not modify the input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name, param)\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          parameter (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  share_memory(self)\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device)\n",
      " |      \n",
      " |      .. function:: to(dtype)\n",
      " |      \n",
      " |      .. function:: to(device, dtype)\n",
      " |      \n",
      " |      It has similar signature as :meth:`torch.Tensor.to`, but does not take\n",
      " |      a Tensor and only takes in floating point :attr:`dtype` s. In\n",
      " |      particular, this method will only cast the floating point parameters and\n",
      " |      buffers to :attr:`dtype`. It will still move the integral parameters and\n",
      " |      buffers to :attr:`device`, if that is given. See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point type of\n",
      " |              the floating point parameters and buffers in this module\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |  \n",
      " |  train(self, mode=True)\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self, dst_type)\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self)\n",
      " |      Sets gradients of all model parameters to zero.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nn.LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(hidden_size=100,input_size=20, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4  5]\n",
      " [ 6  7  8  9 10 11]\n",
      " [12 13 14 15 16 17]\n",
      " [18 19 20 21 22 23]]\n",
      "[[  0  -1  -2  -3  -4  -5]\n",
      " [ -6  -7  -8  -9 -10 -11]\n",
      " [-12 -13 -14 -15 -16 -17]\n",
      " [-18 -19 -20 -21 -22 -23]]\n",
      "[[15 14 13 12 11 10]\n",
      " [ 9  8  7  6  5  4]\n",
      " [ 3  2  1  0 -1 -2]\n",
      " [-3 -4 -5 -6 -7 -8]]\n",
      "[[15 14 13 12 11 10]\n",
      " [ 9  8  7  6  5  4]\n",
      " [ 3  2  1  0  0  0]\n",
      " [ 0  0  0  0  0  0]]\n",
      "tensor([[ 15,  14,  13,  12,  11,  10],\n",
      "        [  9,   8,   7,   6,   5,   4],\n",
      "        [  3,   2,   1,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0]])\n"
     ]
    }
   ],
   "source": [
    "A=np.arange(24).reshape(4,6)\n",
    "print(A)\n",
    "A=A*(-1)\n",
    "print(A)\n",
    "A=A+15\n",
    "print(A)\n",
    "A = np.maximum(A,0)\n",
    "print(A)\n",
    "A = Variable(torch.LongTensor(A))\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = A==0\n",
    "B.float().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Variable(torch.Tensor(1,4,100))\n",
    "out=lstm(emb(a[:,0].unsqueeze(1)), (c,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 100])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 20])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(a).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(['a','a','a','a','a','b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "from collections import Counter\n",
    "import torch\n",
    "import glob\n",
    "from spacy import attrs\n",
    "\n",
    "\n",
    "word2idx = np.load('word2idx.npy').item()\n",
    "vocab_size = len(word2idx)\n",
    "batch_size = 1000\n",
    "\n",
    "nlp = spacy.load('en') # loads default English object\n",
    "cnn_dir = '/home/jatin/intern/internenv/cnn/stories/'\n",
    "cnn_pre_dir = '/home/jatin/intern/internenv/cnn/preprocessed_stories/'\n",
    "\n",
    "file_list = [os.path.join(cnn_dir,file) for file in os.listdir(cnn_dir)]\n",
    "total_files = len(file_list)\n",
    "files_read = 0\n",
    "count = 0\n",
    "for file in file_list[0:100]:\n",
    "    with open(file) as f:\n",
    "        text = f.read()\n",
    "#       print(text)\n",
    "        text = text.lower()\n",
    "        text = text.replace('\\n\\n',' ')\n",
    "        text = text.replace('(cnn)','')\n",
    "        text = text.split(\"@highlight\")\n",
    "        body = text[0]\n",
    "        body_words = body.split(' ')\n",
    "        summaries = ' . '.join(text[1:])+' .'\n",
    "        summary_words = summaries.split(' ')\n",
    "        unique_words = list(set(body_words+summary_words))\n",
    "        temp_dict = dict()\n",
    "        oovs = 0\n",
    "        for w in unique_words:\n",
    "            try:\n",
    "                temp_dict[w] = word2idx[w]\n",
    "            except KeyError:\n",
    "                oovs+=1\n",
    "                temp_dict[w] = oovs+vocab_size\n",
    "        body_idx = [str(temp_dict[x]) for x in body_words]\n",
    "        summary_idx = [str(temp_dict[x]) for x in summary_words]\n",
    "        out = ' '.join(body_idx)+'::'+' '.join(summary_idx)\n",
    "        out_file = file.replace('/stories/','/preprocessed_stories/')\n",
    "    with open(out_file,'w') as f:\n",
    "        f.write(out)\n",
    "    count+=1\n",
    "    if count%100==0:\n",
    "        print(count)\n",
    "\n",
    "\n",
    "# \t\tdoc = nlp(text)\n",
    "\n",
    "\n",
    "# counter = Counter()\n",
    "# while (files_read<total_files):\n",
    "#     word_list = []\n",
    "#     batch_files = file_list[files_read:min(files_read+1000,total_files)]\n",
    "#     for file_name in batch_files:\n",
    "#         with open(file_name) as f:\n",
    "#             text = f.read()\n",
    "#             text = text.lower()\n",
    "#             text = text.replace('\\n\\n',' ')\n",
    "#             text = text.replace('(cnn)','')\n",
    "#             text = text.split(\"@highlight\")\n",
    "#             body = text[0]\n",
    "#             doc = list(nlp(body))\n",
    "#             word_list.extend([x.text for x in doc])\n",
    "\n",
    "#     counter = counter + Counter(word_list)\n",
    "#     files_read+=len(batch_files)\n",
    "#     print(\"%d files read so far...\" % files_read)\n",
    "#     word2idx = {tup[0]: i for i,tup in enumerate(counter.most_common(vocab_size))}\n",
    "#     np.save('word2idx.npy',word2idx)\n",
    "# print(\"All merged!\")\n",
    "# word2idx = {tup[0]: i for i,tup in enumerate(counter.most_common(vocab_size))}\n",
    "# np.save('word2idx.npy',word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
